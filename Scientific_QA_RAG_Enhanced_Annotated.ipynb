{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\udde0 Scientific Q&A Assistant using Gen AI (RAG Pipeline)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\udde0 Scientific Q&A Assistant using Gen AI (RAG Pipeline)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udccc 1. Install Dependencies\n", "!pip install -q openai arxiv wikipedia sentence-transformers langchain matplotlib langchain-community"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \ud83d\udccc Step 1: Install all necessary dependencies\nWe install OpenAI, arXiv, Wikipedia, LangChain, Sentence Transformers, and visualization libraries."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udccc 2. Import Libraries\n", "import requests\n", "import arxiv\n", "import wikipedia\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "from sentence_transformers import SentenceTransformer\n", "from langchain.chat_models import ChatOpenAI\n", "from langchain.schema import HumanMessage"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \ud83d\udccc Step 2: Import required Python libraries\nThese include APIs for data retrieval, embedding models, and plotting tools."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udccc 3. Load Sentence Transformer Embedder\n", "embedder = SentenceTransformer('all-MiniLM-L6-v2')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \ud83d\udccc Step 3: Load the SentenceTransformer model\nThis model is used to create embeddings for scientific text data."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udccc 4. Fetch Articles from PubMed (Entrez API)\n", "def fetch_pubmed_articles(query, max_results=5):\n", "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term={query}&retmax={max_results}&retmode=json\"\n", "    ids = requests.get(url).json()[\"esearchresult\"][\"idlist\"]\n", "    summaries = []\n", "    for pid in ids:\n", "        summary_url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id={pid}&retmode=json\"\n", "        summary_data = requests.get(summary_url).json()\n", "        if pid in summary_data[\"result\"]:\n", "            title = summary_data[\"result\"][pid].get(\"title\", \"\")\n", "            source = summary_data[\"result\"][pid].get(\"source\", \"\")\n", "            summaries.append({\"source\": \"PubMed\", \"title\": title, \"summary\": source})\n", "    return summaries"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \ud83d\udccc Step 4: Define a function to fetch abstracts and metadata from PubMed\nUses Entrez API to search and retrieve summaries."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udccc 5. Fetch Articles from Wikipedia\n", "def get_wikipedia_background(topic):\n", "    try:\n", "        summary = wikipedia.summary(topic, sentences=5)\n", "        return [{\"source\": \"Wikipedia\", \"title\": topic, \"summary\": summary}]\n", "    except Exception:\n", "        return []"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \ud83d\udccc Step 5: Define a function to retrieve a short summary from Wikipedia\nUses the `wikipedia` Python package."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udccc 6. Fetch Articles from arXiv\n", "def fetch_arxiv_articles(query, max_results=5):\n", "    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\n", "    articles = []\n", "    for result in search.results():\n", "        articles.append({\"source\": \"arXiv\", \"title\": result.title, \"summary\": result.summary})\n", "    return articles"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \ud83d\udccc Step 6: Define a function to fetch abstracts from arXiv\nUses the `arxiv` Python SDK to search relevant publications."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udccc 7. Build Combined Report\n", "def build_merged_report(topic, pubmed_limit=5, arxiv_limit=5):\n", "    pubmed = fetch_pubmed_articles(topic, max_results=pubmed_limit)\n", "    arxiv_articles = fetch_arxiv_articles(topic, max_results=arxiv_limit)\n", "    wiki = get_wikipedia_background(topic)\n", "    return pubmed + arxiv_articles + wiki"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \ud83d\udccc Step 7: Merge articles from PubMed, arXiv, and Wikipedia\nCreates a unified list of relevant articles for the topic."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udccc 8. Summarization with Grounding using GPT\n", "def structured_summary_with_grounding(texts):\n", "    joined_context = \"\\n\".join([f\"[{i}] {t['summary']}\" for i, t in enumerate(texts)])\n", "    citations = \"\\n\".join([f\"[{i}] {t['title']} - {t['source']}\" for i, t in enumerate(texts)])\n", "    messages = [\n", "        HumanMessage(content=f\"Answer the question based only on the texts below. Cite each claim using [number].\\n\\n{texts}\\n\\nCitations:\\n{citations}\")\n", "    ]\n", "    chat = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n", "    return chat(messages).content"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \ud83d\udccc Step 8: Generate a grounded scientific answer using OpenAI GPT\nGenerates a summary based only on retrieved article content with references."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udccc 9. Visualization of Source Contributions\n", "def visualize_results(results):\n", "    df = pd.DataFrame(results)\n", "    source_counts = df['source'].value_counts()\n", "    source_counts.plot(kind='bar', color='skyblue', title='Source Distribution')\n", "    plt.ylabel('Count')\n", "    plt.xlabel('Source')\n", "    plt.grid(axis='y')\n", "    plt.tight_layout()\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \ud83d\udccc Step 9: Visualize the number of articles fetched from each source\nDisplays a bar chart showing how many items came from PubMed, arXiv, and Wikipedia."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udccc 10. Interactive Workflow\n", "if __name__ == \"__main__\":\n", "    topic = input(\"\ud83d\udd0d Enter a research topic: \")\n", "    merged_data = build_merged_report(topic)\n", "    visualize_results(merged_data)\n", "    print(\"\\n\\n\ud83d\udcda Sources Fetched:\")\n", "    for doc in merged_data:\n", "        print(f\"- {doc['source']}: {doc['title'][:60]}\")\n", "    print(\"\\n\\n\ud83e\udde0 Answer Summary:\")\n", "    final_summary = structured_summary_with_grounding(merged_data)\n", "    print(final_summary)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 5}